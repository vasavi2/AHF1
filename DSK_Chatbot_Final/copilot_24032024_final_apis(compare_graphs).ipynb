{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Modules\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Modules Related to PDF Pre-Processing\n",
    "import fitz\n",
    "import os\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from fpdf import FPDF\n",
    "import PIL.Image\n",
    "\n",
    "# Modules Related to PDF Processing\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA,StuffDocumentsChain ,LLMChain\n",
    "\n",
    "# Modules Related to SQL Processing \n",
    "import psycopg2\n",
    "import urllib\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#smple\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL RELATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules Related to AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# import json\n",
    "# with open('Azure.json') as f:\n",
    "#     Azure=json.load(f)\n",
    "\n",
    "\n",
    "# # Define AzureOpenAI Model and Embedding Model\n",
    "# chat_model = AzureChatOpenAI(openai_api_base=Azure['api_base'],\n",
    "#                   azure_deployment=Azure['deployment_name'],\n",
    "#                   openai_api_key=Azure['api_key'],\n",
    "#                   openai_api_type=Azure['api_type'],\n",
    "#                   openai_api_version=Azure[\"api_version\"],        \n",
    "#                     seed=1234,\n",
    "#                  temperature = 0)\n",
    "\n",
    "\n",
    "\n",
    "# embeddings =OpenAIEmbeddings(\n",
    "#     deployment=Azure['embedding_name'],\n",
    "#     model=Azure['embedding_model'],\n",
    "#     openai_api_base=Azure['api_base'],\n",
    "#     openai_api_type=Azure['api_type'],\n",
    "#     openai_api_key=Azure['api_key'],\n",
    "#     chunk_size = 10\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\40019115\\geminiAPI\\DSK_Chatbot\n"
     ]
    }
   ],
   "source": [
    "#Load The PDF Documents\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "files_name1 = \"Centrifugal Air Compressor Troubleshooting Guide 2.pdf\"\n",
    "files_name2 = \"17DA-3SS 1.pdf\"\n",
    "files_name3 = \"Air_Compressor_Startup_Shutdown_Normal_Operation 1.pdf\"\n",
    "\n",
    "a = os.path.join(path, files_name1)\n",
    "b = os.path.join(path, files_name2)\n",
    "c = os.path.join(path, files_name3)\n",
    "\n",
    "loaders = [\n",
    "\n",
    "#      PyPDFLoader(\"C:/Users/40019115/geminiAPI/Centrifugal Air Compressor Troubleshooting Guide 2.pdf\"),\n",
    "#     PyPDFLoader(\"C:/Users/40019115/geminiAPI//17DA-3SS 1.pdf\"),\n",
    "#     PyPDFLoader(\"C:/Users/40019115/geminiAPI//Air_Compressor_Startup_Shutdown_Normal_Operation 1.pdf\")\n",
    "    PyPDFLoader(a),\n",
    "    PyPDFLoader(b),\n",
    "    PyPDFLoader(c),\n",
    "    \n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "# print(\"docs---->\",docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Gemini API Key\n",
    "GOOGLE_API_KEY = 'AIzaSyBIBaI7Cr-bINi-cRK9BHa2rUMK2MpqONQ'\n",
    "# Define Model and Embedding Model\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=GOOGLE_API_KEY)\n",
    "chat_model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=400\n",
    "\n",
    "# Define Splitter and Vector DB\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-----> \u001b[1mGoogleGenerativeAI\u001b[0m\n",
      "Params: {}\n",
      "embeddings------> model='models/embedding-001' task_type=None google_api_key=SecretStr('**********') client_options=None transport=None\n",
      "chat_model-----> model='gemini-pro' client=genai.GenerativeModel(\n",
      "    model_name='models/gemini-pro',\n",
      "    generation_config={},\n",
      "    safety_settings={},\n",
      "    tools=None,\n",
      "    system_instruction=None,\n",
      ") google_api_key=SecretStr('**********')\n",
      "chunk_size-----> 400\n"
     ]
    }
   ],
   "source": [
    "print(\"Model----->\",model)\n",
    "print(\"embeddings------>\",embeddings)\n",
    "print(\"chat_model----->\",chat_model)\n",
    "print(\"chunk_size----->\",chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Document and Store it in Vector DB\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\40019115\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"purpose of TWO STAGE CL2 COMPRESSOR STARTUP AND SHUTDOWN (PSM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2/8/23, 9:22 AM TWO ST AGE CL2 COMPRESSOR ST ARTUP AND SHUTDOWN (PSM)\\nfile:///C:/Users/203001868/Desktop/BKV Procedure Management System/1239.HTML 1/301. Purpose\\n\\xa0\\nA. To provide instructions for the following operations of the Two\\nStage CL2 Compressor:\\n\\xa0\\n1.\\xa0\\xa0\\xa0\\xa0\\xa0 Pre-Start Lube System Check\\n\\xa0\\n2.\\xa0\\xa0\\xa0\\xa0\\xa0 Compressor Start-Up\\n\\xa0\\n3.\\xa0\\xa0\\xa0\\xa0\\xa0 Compressor Shutdown\\n\\xa0\\n4.\\xa0\\xa0\\xa0\\xa0\\xa0 Compressor Testing\\n\\xa0\\nB. Field Operators and Console Operators are responsible for the\\nperformance of this procedure.\\n\\xa0\\nC. If All 3 of the Chlorine compressors are shutdown, the 12-inch\\nCl2 gas outlet valves on the Dry Mist Eliminators, XV1541035\\n(Diaphragm) and/or XV3541057 (Membrane) will CLOSE as well\\nas XV2561049 to Liquefaction.\\n\\xa0\\n2. Safety and Health Hazards\\n\\xa0\\nReview Safety Data Sheet and Brine Recovery PPE Requirements for:\\n\\xa0\\nChlorine – (CL2) BKV-1558\\nPerchloroethylene Liquid – (Perchlor) BVK-1250\\nNitrogen (N2) – BKV-1191\\n\\xa0\\nSafety Equipment:\\n\\xa0\\nA.\\xa0\\xa0\\xa0 An SCBA is required, as well as the normal safety gear.\\n\\xa0\\nOther Hazards:\\n\\xa0\\nA.\\xa0\\xa0\\xa0 Failure to check facemask for leakage, after donning SCBA,\\ncould result in inhalation of toxic or inert gas concentrations.\\nRemoval of the SCBA facemask in an Enclosure could be life\\nthreatening.\\n\\xa0', metadata={'source': 'C:\\\\Users\\\\40019115\\\\geminiAPI\\\\DSK_Chatbot\\\\Air_Compressor_Startup_Shutdown_Normal_Operation 1.pdf', 'page': 0}),\n",
       " Document(page_content='2/8/23, 9:22 AM TWO ST AGE CL2 COMPRESSOR ST ARTUP AND SHUTDOWN (PSM)\\nfile:///C:/Users/203001868/Desktop/BKV Procedure Management System/1239.HTML 28/30InitialsProcedure Steps Keypoint/Notes\\n\\xa0 16. NOTIFY Console Operator to\\nstart compressor.\\xa0\\n\\xa0\\n11. Post-Performance Activities\\n\\xa0\\nInitials Post-Performance Activities Keypoint/Notes\\n\\xa0 1.\\xa0\\xa0\\xa0\\xa0\\xa0 CONTINUE purging Compressor\\nuntil Compressor is ready to be\\nreturned to service.\\xa0\\n\\xa0 2.\\xa0\\xa0\\xa0\\xa0\\xa0 If necessary, INITIATE work\\norders for any repa ir or\\nreplacement parts that may be\\nneeded.\\xa0\\n\\xa0 3.\\xa0\\xa0\\xa0\\xa0\\xa0 NOTIFY Console Operator to\\nREMOVE your name(s) from the\\nChlorine Compressor Enclosure\\nin accordance with #1481,\\n“BRINE RECOVERY ENCLOSURE\\nENTRY POLICY”.\\xa0\\n\\xa0 4.\\xa0\\xa0\\xa0\\xa0\\xa0 RETURN SCBA to its proper\\nstorage area in a clean and dry\\ncondition.\\xa0\\n\\xa0 5.\\xa0\\xa0\\xa0\\xa0\\xa0 REPORT AND VERIFY that any\\nde\\x00ciencies encountered during\\nthe performance of this SOP are\\nresolved.\\xa0\\n\\xa0\\n12. Records\\n\\xa0\\nA. DOCUMENT time AND actions taken in Control Room Log Book\\nin accordance with #1166, “BRINE RECOVERY LOG BOOK”.\\n\\xa0\\n13. References\\n\\xa0\\nA. USE:\\n\\xa0\\n1.\\xa0\\xa0\\xa0\\xa0\\xa0 #1166, “BRINE RECOVERY LOG BOOK”.\\n\\xa0\\n2.\\xa0\\xa0\\xa0\\xa0\\xa0 #1481, “ENCLOSURE ENTRY POLICY”.\\n\\xa0', metadata={'source': 'C:\\\\Users\\\\40019115\\\\geminiAPI\\\\DSK_Chatbot\\\\Air_Compressor_Startup_Shutdown_Normal_Operation 1.pdf', 'page': 27}),\n",
       " Document(page_content='2/8/23, 9:22 AM TWO ST AGE CL2 COMPRESSOR ST ARTUP AND SHUTDOWN (PSM)\\nfile:///C:/Users/203001868/Desktop/BKV Procedure Management System/1239.HTML 16/30\\xa0\\nInitialsPrerequisites Keypoint/Notes\\nA.\\xa0\\xa0\\xa0\\xa0 Preliminary Actions:\\n\\xa0 1.\\xa0\\xa0\\xa0\\xa0\\xa0 None \\xa0\\nB.\\xa0\\xa0\\xa0\\xa0 Personnel Requirements:\\n\\xa0 1.\\xa0\\xa0\\xa0\\xa0\\xa0 ENSURE that at least one Field\\nOperator, one Console Operator\\nAND a Team Lead are available to\\nperform this procedu re.\\xa0\\nC.\\xa0\\xa0\\xa0\\xa0 Tools, Equipment, and Supplies:\\n\\xa0 1.\\xa0\\xa0\\xa0\\xa0\\xa0 Hand Held Radio. \\xa0\\n\\xa0 2.\\xa0\\xa0\\xa0\\xa0\\xa0 Hand Tools and a Flashlight. \\xa0\\nD.\\xa0\\xa0\\xa0\\xa0 Field Preparations:\\n\\xa0 1.\\xa0\\xa0\\xa0\\xa0\\xa0 Each Field Operator to NOTIFY\\nConsole Operator to PLACE their\\nname on Chlorine Compressor\\nEnclosure in accordance with\\nQualtrax #1481 “ENCLOSURE\\nENTRY POLICY”.\\xa0\\n\\xa0 2.\\xa0\\xa0\\xa0\\xa0\\xa0 ENSURE suﬃcient Scrubber\\ncharges are in the Vent Scrubber\\n(minimum of 15% Caustic on\\nVent Scrubber).\\xa0\\n\\xa0 3.\\xa0\\xa0\\xa0\\xa0\\xa0 Team Lead RECONCILE with\\nMaintenance Lead that all Work\\nOrders initiated for Chlorine\\nSystem are completed AND all\\nlocks and piping blinds are\\nremoved.\\xa0\\n\\xa0 4.\\xa0\\xa0\\xa0\\xa0\\xa0 If the compressor has been\\nlocked out for maintenance then\\ncontact a maintenance person to\\nverify the proper nitrogen l \\x00ow\\non the compressor seals.\\xa0\\n\\xa0 5.\\xa0\\xa0\\xa0\\xa0\\xa0 Operator to VERIFY that the\\nmain electrical breaker for the\\ncompressor is “CLOSED” at MCC.\\xa0', metadata={'source': 'C:\\\\Users\\\\40019115\\\\geminiAPI\\\\DSK_Chatbot\\\\Air_Compressor_Startup_Shutdown_Normal_Operation 1.pdf', 'page': 15}),\n",
       " Document(page_content='2/8/23, 9:22 AM TWO ST AGE CL2 COMPRESSOR ST ARTUP AND SHUTDOWN (PSM)\\nfile:///C:/Users/203001868/Desktop/BKV Procedure Management System/1239.HTML 13/30InitialsProcedure Steps Keypoint/Notes\\n\\xa0 F.\\xa0\\xa0\\xa0\\xa0 When the discharge pressure\\nreads approximately 7 to 10\\npsig, then STOP Closing the\\nvalves.\\xa0\\n\\xa0 G.\\xa0\\xa0\\xa0 The two stage Cl2\\nCompressor is now running\\non nitrogen to the Vent\\nHeader.\\xa0\\n\\xa0\\nD.\\xa0\\xa0\\xa0 Normal Operating Tasks\\n\\xa0\\n1.\\xa0 Chlorine Area Operator will don a SCBA once each week and check\\nthe oil level of each compressor as well as listen for any unusual\\nnoise or vibration.\\n2.\\xa0 Console Operator will monitor the \\x00ow, amps and overall setup of\\nall running Cl2 compressors.\\n\\xa0\\n6. Temporary Operations\\n\\xa0\\nTemporary Operations require a TMOC.\\n\\xa0\\n7. Normal Shutdown\\n\\xa0\\nA.\\xa0\\xa0\\xa0 Prerequisites\\n\\xa0\\nInitialsPrerequisites Keypoint/Notes\\n\\xa0 1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 SCBA trained. \\xa0\\n\\xa0 2.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Enclosure entry trained. \\xa0\\n\\xa0\\nB.\\xa0\\xa0\\xa0 Acceptance Criteria, Operating Limits, Consequences of\\nDeviation and Steps Taken to Avoid Deviation\\n\\xa0\\nWARNING:\\xa0Deviation from this procedure could result in the\\nrelease of toxic material, injury or death to personnel, damage\\nto equipment or an environmental non-conformance.\\n\\xa0\\nSee COD Table under Normal Operations\\n\\xa0\\nC.\\xa0\\xa0\\xa0 Procedure', metadata={'source': 'C:\\\\Users\\\\40019115\\\\geminiAPI\\\\DSK_Chatbot\\\\Air_Compressor_Startup_Shutdown_Normal_Operation 1.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "  provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "  Context:\\n {context}?\\n\n",
    "  Question: \\n{question}\\n \n",
    "  Answer: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_pdf(user_query):\n",
    "    print(\"get_answer_from_pdf------>\")\n",
    "    retrieved_docs = retriever.get_relevant_documents(user_query)\n",
    "    context = retrieved_docs[0].page_content\n",
    "    qa_retreival = LLMChain(llm=chat_model,prompt=prompt)\n",
    "    response = qa_retreival.invoke({\"context\": context, \"question\": user_query})\n",
    "    print(\"response_text\",response[\"text\"])\n",
    "    return response[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "conn = sqlite3.connect('database_new.db')\n",
    "\n",
    "\n",
    "# # username = \"postgres\"\n",
    "# # password = \"A#F@2023\"\n",
    "# # host = \"35.208.159.230\"\n",
    "# # port = \"8090\"\n",
    "# # mydatabase = \"assetdb\"\n",
    "\n",
    "\n",
    "\n",
    "# host=\"localhost\"\n",
    "# port=\"5432\"\n",
    "# mydatabase=\"AHF_Project\"\n",
    "# password=\"root\"\n",
    "# username=\"postgres\"\n",
    " \n",
    "# #establishing the connection\n",
    "# conn = psycopg2.connect(database=\"AHF_Project\",user='postgres', password='root', host='localhost', port= \"5432\")\n",
    "# print(\"connection\",conn)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# encoded_password = urllib.parse.quote(\"root\")\n",
    " \n",
    "# db = SQLDatabase.from_uri(\n",
    "#     f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{mydatabase}\"\n",
    "# )\n",
    " \n",
    "# agent_executor = create_sql_agent(\n",
    "#     llm=model,\n",
    "#     toolkit=SQLDatabaseToolkit(db=db, llm=model),\n",
    "#     verbose=True,\n",
    "#     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# host=\"localhost\"\n",
    "# port=\"5432\"\n",
    "# mydatabase=\"AHF_Project\"\n",
    "# password=\"root\"\n",
    "# username=\"postgres\"\n",
    " \n",
    "# #establishing the connection\n",
    "# conn = psycopg2.connect(database=\"AHF_Project\",user='postgres', password='root', host='localhost', port= \"5432\")\n",
    "# print(\"connection\",conn)\n",
    "\n",
    " \n",
    "# db = SQLDatabase.from_uri(\n",
    "#     f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{mydatabase}\"\n",
    "# )\n",
    " \n",
    "# agent_executor = create_sql_agent(\n",
    "#     llm=model,\n",
    "#     toolkit=SQLDatabaseToolkit(db=db, llm=model),\n",
    "#     verbose=True,\n",
    "#     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_values=\"\"\n",
    "x_values_plot=[]\n",
    "y_values_plot=[]\n",
    "\n",
    "x1_values_compare_plot=[]\n",
    "x11_values_compare_plot=[]\n",
    "y1_values_compare_plot=[]\n",
    "\n",
    "\n",
    "\n",
    "x2_values_compare_plot=[]\n",
    "x22_values_compare_plot=[]\n",
    "y2_values_compare_plot=[]\n",
    "\n",
    "def get_sql_query_from_excel(query):\n",
    "    n=0\n",
    "    global x_values_plot\n",
    "    global y_values_plot\n",
    "    global x1_values_compare_plot\n",
    "    global x11_values_compare_plot\n",
    "    global y1_values_compare_plot\n",
    "    \n",
    "    global x2_values_compare_plot\n",
    "    global x22_values_compare_plot\n",
    "    global y2_values_compare_plot\n",
    "\n",
    "    \n",
    "    loader = UnstructuredExcelLoader(r\"C:\\Users\\40019115\\geminiAPI\\Table Description.xlsx\")\n",
    "    docs = loader.load()\n",
    "    vectorstore_db = FAISS.from_documents(docs,embeddings)\n",
    "    embeddings_vector = embeddings.embed_query(query)\n",
    "    docs = vectorstore_db.similarity_search_by_vector(embeddings_vector)\n",
    "    prompt_template = \"\"\"\n",
    "    You are the best in converting the natural language to sql and give sql queries. You can ignore the word 'plot' in the query. The task you should perform here is to give an accurate \n",
    "    sql query for the given question the table details are provided in the context so give me the accurate sql query.\n",
    "    If the user ask question regarding plot and forecast give me the sql query to get the value of the column and the\n",
    "    timestamp which the user is asking don't give any answer for plotting just need sql query and same for forecast.\n",
    "    take in consideration that the value in the timestamp column are not arranged properly. Ignore rest of the text in response and provide with SQL query.\n",
    "    you have to give only the sql query as response and ignore rest of the text. In sql query always have timestamp after 'select'. If needed use 'ONLY' joins and use table name before columns..\\n\\n\n",
    "    \n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n \n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    context = docs[0]\n",
    "    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
    "    qa_retreival = LLMChain(llm=chat_model,prompt=prompt)\n",
    "    response = qa_retreival.invoke({\"context\": context, \"question\": query})\n",
    "    query = response[\"text\"]\n",
    "    query = response['text']\n",
    "    s = 'select'.upper()\n",
    "    words = query.split(s)\n",
    "    query = 'select' + words[1]\n",
    "    print(query)\n",
    "\n",
    "    \n",
    "    if 'join' in query.replace('\\n','').split(' ') or 'join'.upper() in query.replace('\\n','').split(' '):\n",
    "        n=1\n",
    "#         user_question = \"Compare and Plot time trend graph for Comp_Stage_1_Flow_217FC7343A, Comp_DGS_DE_Primery_Seal_Vent_Diff_217PDI7162 from 2nd feb to 28th feb\"\n",
    "\n",
    "        print(\"1\")\n",
    "        table_name = query.split(' ')[5]\n",
    "        st = query.split()\n",
    "        given_word = table_name +'.timestamp'\n",
    "        for i, word in enumerate(st):\n",
    "            if word == 'timestamp' or word == 'timestamp'.upper() or word == 'timestamp'.capitalize():                                                                                                                                                                                \n",
    "                st[i] = given_word\n",
    "                \n",
    "\n",
    "        query = ' '.join(st)\n",
    "        \n",
    "        w = query.split(' ')[5]+'.'+query.split(' ')[1].split(',')[0]\n",
    "        q = query.replace(query.split(' ')[1].split(',')[0], w,1)\n",
    "        alert_df1 = pd.read_sql_query(q, con=conn)\n",
    "        alert_df1.sort_values('Timestamp',inplace=True)\n",
    "        alert_df1.set_index('Timestamp',inplace=True)\n",
    "        \n",
    "#         print(\"1st column\",len(alert_df1[alert_df1.columns[0]].to_list())) #1st column\n",
    "#         print(alert_df1[alert_df1.columns[0]].to_list()) #1st column\n",
    "        \n",
    "        #x1_compare_values------>\n",
    "        x1_values_compare_plot.append(alert_df1[alert_df1.columns[0]].to_list())\n",
    "        x11_values_compare_plot.append(alert_df1[alert_df1.columns[1]].to_list())\n",
    "        y1_values_compare_plot.append(alert_df1.index.to_list())\n",
    "        \n",
    "        plt.figure(figsize=(15,5))\n",
    "        alert_df1[alert_df1.columns[0]].plot(legend=True,color='orange')\n",
    "        ax2 = plt.twinx()\n",
    "        alert_df1[alert_df1.columns[1]].plot(legend=True,color='green')\n",
    "        plt.ylabel('VALUES for timeseries 1')\n",
    "        ax2.set_ylabel('VALUES for timeseries 2')\n",
    "    else:\n",
    "        n=2\n",
    "        alert_df1 = pd.read_sql_query(query, con=conn)\n",
    "        # print(len(alert_df1.columns))\n",
    "        if len(alert_df1.columns) > 2:\n",
    "            print(\"2\")\n",
    "#             user_question = \"Compare and Plot time trend graph for Comp_Stage_1_Flow_217FC7343A, Comp_Stage_1_Suction_Press_217PI7003 from 2nd feb to 28th feb\"\n",
    "            alert_df1.sort_values('Timestamp',inplace=True)\n",
    "            alert_df1.set_index('Timestamp',inplace=True)\n",
    "        \n",
    "        \n",
    "            print(alert_df1[alert_df1.columns[0]].to_list()[:100]) #1st column\n",
    "            print(alert_df1[alert_df1.columns[1]].to_list()[:100]) #second column\n",
    "            print(alert_df1.index.to_list()[:100]) #y axis(timestamp)\n",
    "            \n",
    "            \n",
    "            \n",
    "            x2_values_compare_plot.append(alert_df1[alert_df1.columns[0]].to_list())\n",
    "            x22_values_compare_plot.append(alert_df1[alert_df1.columns[1]].to_list())\n",
    "            y2_values_compare_plot.append(alert_df1.index.to_list())\n",
    "            \n",
    "            \n",
    "            \n",
    "            plt.figure(figsize=(15,5))\n",
    "            alert_df1[alert_df1.columns[0]].plot(legend=True,color='orange')\n",
    "            ax2 = plt.twinx()\n",
    "            alert_df1[alert_df1.columns[1]].plot(legend=True,color='green')\n",
    "            plt.ylabel('VALUES for timeseries 1')\n",
    "            ax2.set_ylabel('VALUES for timeseries 2')\n",
    "        \n",
    "        else:\n",
    "            n=3\n",
    "            print(\"3\")\n",
    "            alert_df1.sort_values('Timestamp',inplace=True)\n",
    "            alert_df1.set_index('Timestamp',inplace=True)\n",
    "            \n",
    "            x_values_s=alert_df1.index.to_list()\n",
    "            y_values_s=alert_df1[alert_df1.columns[0]].to_list()\n",
    "            \n",
    "            \n",
    "            x_values_plot.append(x_values_s)\n",
    "            y_values_plot.append(y_values_s)\n",
    "            \n",
    "            \n",
    "            graph_data={\n",
    "                'x':x_values_plot,\n",
    "                'y':y_values_plot\n",
    "            }\n",
    "            graph_values=graph_data\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "#             print(alert_df1[alert_df1.columns[0]].to_list())\n",
    "#             print(alert_df1.index.to_list())\n",
    "#             plt.figure(figsize=(15,5))\n",
    "#             alert_df1[alert_df1.columns[0]].plot(legend=True,color='orange')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             #Plot time trend graph for Comp_Stage_1_Flow_217FC7343A?\n",
    "#             print(\"3\")        \n",
    "# #             alert_df1.sort_values('Timestamp',inplace=True)\n",
    "# #             alert_df1.set_index('Timestamp',inplace=True)\n",
    "#             plt.figure(figsize=(15,5))\n",
    "#             print(\"alert_df1\",alert_df1)\n",
    "        \n",
    "        \n",
    "        \n",
    "# #             alert_df1[alert_df1.columns[0]].plot(legend=True,color='orange')\n",
    "            \n",
    "# #             print(\"sample1--->\",alert_df1.columns[0])\n",
    "# #             print(\"sample2--->\",alert_df1[alert_df1.columns[0]])\n",
    "            \n",
    "# #             timestamps = alert_df1[alert_df1.columns[0]].index.tolist()\n",
    "# #             values = alert_df1[alert_df1.columns[0]].tolist()\n",
    "\n",
    "# #             print(\"Timestamps: \", timestamps)\n",
    "# #             print(\"Values: \", values)\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "#             column_headers_names = list(alert_df1.columns)\n",
    "#             column_headers = list(alert_df1.columns.values)\n",
    "            \n",
    "            \n",
    "#             print(\"column_headers_names\",column_headers_names)\n",
    "#             print(\"column_headers\",column_headers)\n",
    "            \n",
    "#             timestamp_Values=alert_df1[column_headers[0]].to_list()\n",
    "#             y_Values=alert_df1[column_headers[1]].to_list()\n",
    "            \n",
    "            \n",
    "# #             timestamp_Values=alert_df1[column_headers[1]].tolist()\n",
    "# #             iso_format=[i.isoformat() for i in timestamp_Values]\n",
    "# #             print(\"x value without iso converter\",timestamp_Values)\n",
    "\n",
    "                  \n",
    "\n",
    "#             x_values_plot.append(timestamp_Values)\n",
    "# #             y_values_plot.append(alert_df1[column_headers[0]].to_list())\n",
    "#             y_values_plot.append(y_Values)\n",
    "            \n",
    "#             print(\"x values--->\",timestamp_Values)\n",
    "#             print(\"y values--->\",y_Values)\n",
    "                            \n",
    "                \n",
    "#             graph_data={\n",
    "#                     'x':timestamp_Values,\n",
    "#                     'y':y_Values\n",
    "#                 }\n",
    "#             graph_values=graph_data\n",
    "            \n",
    "            \n",
    "            \n",
    "#             alert_df = pd.read_sql_query(query, con=conn)\n",
    "# #                 print(\"alert\",alert_df)\n",
    "#             column_headers_names = list(alert_df.columns)\n",
    "#             column_headers = list(alert_df.columns.values)\n",
    "#             print(\"column_headers_names\",column_headers_names)\n",
    "#             print(\"column_headers\",column_headers)\n",
    "\n",
    "#             fig,ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "#             timestamp_Values=alert_df[column_headers[1]].tolist()\n",
    "# #             iso_format=[i.isoformat() for i in timestamp_Values]\n",
    "\n",
    "\n",
    "#             x_values_plot.append(timestamp_Values)\n",
    "#             y_values_plot.append(alert_df[column_headers[0]].to_list())\n",
    "\n",
    "#             graph_data={\n",
    "#                 'x':x_values_plot,\n",
    "#                 'y':alert_df[column_headers[0]].to_list()\n",
    "#             }\n",
    "# #                 print(\"graph_data->\",graph_data)\n",
    "#             #trying..................................\n",
    "#             graph_values=graph_data\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature_values=[]\n",
    "y_feature_values=[]\n",
    "base64_encoded_img1=\"\"\n",
    "\n",
    "\n",
    "def sql_processing(user_question):\n",
    "    global base64_encoded_img1\n",
    "    global x_feature_values\n",
    "    global y_feature_values\n",
    "    x = 0\n",
    "    count = 0\n",
    " \n",
    "    # Define your dictionary of questions and queries, removing underscores from keys\n",
    "    questions = {\n",
    "        \"asset_name_sensorgroup_name\": \"SELECT sensorgroup_name, asset_name, COUNT(*) AS num_alerts, MAX(start_time) AS start_time FROM alert_table_consolidate WHERE alert_status='closed' GROUP BY sensorgroup_name, asset_name ORDER BY num_alerts DESC, start_time DESC;\",\n",
    "        \"assets_sensorgroups\": \"SELECT sensorgroup_name, asset_name, COUNT(*) AS num_alerts, MAX(start_time) AS start_time FROM alert_table_consolidate WHERE alert_status='closed' GROUP BY sensorgroup_name, asset_name ORDER BY num_alerts DESC, start_time DESC;\",\n",
    "        \"device_info_start_time\": \"SELECT asset_master.asset_name, sensorgroup_master.name, alert_table_consolidate.start_time FROM alert_table_consolidate JOIN asset_master ON alert_table_consolidate.asset_id = asset_master.id JOIN sensorgroup_master ON alert_table_consolidate.sensorgroup_id = sensorgroup_master.sensorgroup_id ORDER BY start_time DESC LIMIT 10\",\n",
    "        \"forecast_Comp_IB_Bearing_Shaft_Radial_Vib_X_217VI7113A\":\"SELECT Comp_IB_Bearing_Shaft_Radial_Vib_X_217VI7113A,Timestamp from _2K1701_CJB_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_IB_Bearing_Shaft_Radial_Vib_Y_217VI7113B\":\"SELECT Comp_IB_Bearing_Shaft_Radial_Vib_Y_217VI7113B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_IB_Journal_Bearing_Temp_B_217TI7115B\":\"SELECT Comp_IB_Journal_Bearing_Temp_B_217TI7115B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_OB_Bearing_Shaft_Radial_Vib_X_217VI7114A\":\"SELECT Comp_OB_Bearing_Shaft_Radial_Vib_X_217VI7114A,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_OB_Bearing_Shaft_Radial_Vib_Y_217VI7114B\":\"SELECT Comp_OB_Bearing_Shaft_Radial_Vib_Y_217VI7114B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_OB_Journal_Bearing_Temp_B_217TI7116B\":\"SELECT Comp_OB_Journal_Bearing_Temp_B_217TI7116B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_Stage_1_Discharge_Press_217PI7008\":\"SELECT Comp_Stage_1_Discharge_Press_217PI7008,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_Stage_1_Suction_Press_217PI7003\":\"SELECT Comp_Stage_1_Suction_Press_217PI7003,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Comp_Turbine_Speed_A_217SC7001A\":\"SELECT Comp_Turbine_Speed_A_217SC7001A,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Lube_Oil_Press_A_217PI7151A\":\"SELECT Lube_Oil_Press_A_217PI7151A,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Lube_Oil_Supply_Temp_216TI6103\":\"SELECT Lube_Oil_Supply_Temp_216TI6103,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"forecast_Compressor_Stage_1_Performance\":\"SELECT Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_IB_Bearing_Shaft_Radial_Vib_X_217VI7113A\":\"SELECT Comp_IB_Bearing_Shaft_Radial_Vib_X_217VI7113A,Timestamp from _2K1701_CJB_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_IB_Bearing_Shaft_Radial_Vib_Y_217VI7113B\":\"SELECT Comp_IB_Bearing_Shaft_Radial_Vib_Y_217VI7113B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_IB_Journal_Bearing_Temp_B_217TI7115B\":\"SELECT Comp_IB_Journal_Bearing_Temp_B_217TI7115B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_OB_Bearing_Shaft_Radial_Vib_X_217VI7114A\":\"SELECT Comp_OB_Bearing_Shaft_Radial_Vib_X_217VI7114A,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_OB_Bearing_Shaft_Radial_Vib_Y_217VI7114B\":\"SELECT Comp_OB_Bearing_Shaft_Radial_Vib_Y_217VI7114B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_OB_Journal_Bearing_Temp_B_217TI7116B\":\"SELECT Comp_OB_Journal_Bearing_Temp_B_217TI7116B,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_Stage_1_Discharge_Press_217PI7008\":\"SELECT Comp_Stage_1_Discharge_Press_217PI7008,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_Stage_1_Suction_Press_217PI7003\":\"SELECT Comp_Stage_1_Suction_Press_217PI7003,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Comp_Turbine_Speed_A_217SC7001A\":\"SELECT Comp_Turbine_Speed_A_217SC7001A,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Lube_Oil_Press_A_217PI7151A\":\"SELECT Lube_Oil_Press_A_217PI7151A,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Lube_Oil_Supply_Temp_216TI6103\":\"SELECT Lube_Oil_Supply_Temp_216TI6103,Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \"plot_Compressor_Stage_1_Performance\":\"SELECT Timestamp from _2K1701_CJB_raw_raw ORDER BY Timestamp\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Preprocess the user question by converting it to lowercase and removing punctuation\n",
    "    processed_question = re.sub(r\"[^\\w\\s]\", \"\", user_question.lower()).strip()\n",
    "    \n",
    "    # Split the processed question into words\n",
    "    processed_question_words = processed_question.split()\n",
    "    \n",
    "    # Iterate through the dictionary, removing underscores from keywords and preprocessing\n",
    "    for keyword, query in questions.items():\n",
    "        count +=1\n",
    "        # print(\"Count.........\", count)\n",
    "        # Preprocess keyword: remove underscores, convert to lowercase, remove punctuation\n",
    "        processed_keyword = re.sub(r\"[^\\w\\s]\", \"\", re.sub(r\"_\", \" \", keyword.lower())).strip()\n",
    "    \n",
    "        # Split the preprocessed keyword into words\n",
    "        keyword_words = processed_keyword.split()\n",
    "        \n",
    "        data_matching=[]\n",
    "\n",
    "    \n",
    "        # Check if any keyword in the preprocessed keyword is present in the processed question words\n",
    "        if all(word in processed_question_words for word in keyword_words):\n",
    "            # If a match is found, print the query and break the loop\n",
    "            x=1\n",
    "            if(count<3):\n",
    "                curr = conn.cursor()\n",
    "                curr.execute(query)\n",
    "                data = curr.fetchall()\n",
    "                \n",
    "                #Apis------>\n",
    "                m=[]  \n",
    "                print(\"data\",data)\n",
    "                for item in data:\n",
    "                    print(\"item\",item)\n",
    "#                     modified_item = (*item[:-1], item[-1].isoformat())\n",
    "                    m.append(item)\n",
    "#                 print(m)\n",
    "#                 print(\"data--->\",data)\n",
    "                \n",
    "                first_data_length=len(data[0])\n",
    "                \n",
    "                \n",
    "                table_data=dict(zip([str(i) for i in range(first_data_length)],m))\n",
    "                base64_encoded_img1=str(table_data)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 list asset name and sensorgroup name are in alert status?\n",
    "                for row in data:\n",
    "                    data_matching.append(row)\n",
    "                    print(row)\n",
    "                    print(\"\\n\")\n",
    "                \n",
    "                \n",
    "            # elif(count<15):\n",
    "            #     print(count)\n",
    "            #     print(\"inside elif\")\n",
    "            #     df = pd.read_sql_query(query, con=conn)\n",
    "            #     print(df)\n",
    "            #     column_headers = list(df.columns.values)\n",
    "            #     print(column_headers)\n",
    "\n",
    "            #     fig,ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "            #     df.plot(kind='line', x=column_headers[1], y=column_headers[0], color='black', ax=ax)\n",
    "\n",
    "            #     # set the title\n",
    "            #     plt.title('LinePlots')\n",
    "        \n",
    "            #     # show the plot\n",
    "            #     plt.show()\n",
    "            else:\n",
    "                x=2\n",
    "                # print(count)\n",
    "                # print(\"inside if\")\n",
    "                alert_df1 = pd.read_sql_query(query, con=conn)\n",
    "                print(\"<-----------Forecasting-------------->\")\n",
    "                \n",
    "#                 df = alert_df\n",
    "                column_headers = list(alert_df1.columns.values)\n",
    "                first_column = column_headers[0]\n",
    "                second_column = column_headers[1]\n",
    "                \n",
    "                \n",
    "                last_timestamp = alert_df1[second_column].iloc[-1]\n",
    "                model_fi1=ARIMA(alert_df1[first_column],order=(1,1,1))\n",
    "                model_fi1=model_fi1.fit()\n",
    "                \n",
    "                \n",
    "                last_30d_df = alert_df1.tail(2977)\n",
    "                start=len(alert_df1)\n",
    "                end=len(alert_df1)-1+192+96\n",
    "                pred=model_fi1.predict(start=start,end=end,typ='levels').rename('ARIMA Predictions')\n",
    "\n",
    "                forecast_Column = first_column+'_forecast'\n",
    "                forecast_range1=pd.date_range(start=last_timestamp, periods=288,freq='15T')\n",
    "                new_data = {second_column: forecast_range1,forecast_Column: pred}\n",
    "                new_df = pd.DataFrame(new_data)\n",
    "                \n",
    "                print(\"-------***--------\")\n",
    "                print(last_30d_df)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 concatenated_df = pd.concat([last_30d_df, new_df], ignore_index=True)\n",
    "\n",
    "#                 concatenated_df.set_index('Timestamp',inplace=True)\n",
    "\n",
    "                plt.figure(figsize=(15,5))\n",
    "#                 concatenated_df[first_column].plot(color='blue',label=first_column)\n",
    "#                 concatenated_df[forecast_Column].plot(color='orange',label=forecast_Column)\n",
    "            \n",
    "                last_30d_df[first_column].plot(color='blue',label=first_column)\n",
    "                \n",
    "                new_df[forecast_Column].plot(color='orange',label=forecast_Column)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Apis data------>\n",
    "#                 print(\"last_30d_df[timestamp]\",last_30d_df[\"Timestamp\"].tolist())\n",
    "#                 print(\"new df\",new_df[\"Timestamp\"].tolist())\n",
    "                \n",
    "#                 iso_format=[i.isoformat() for i in last_30d_df[\"Timestamp\"].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "                without_iso_format=last_30d_df[\"Timestamp\"].tolist()\n",
    "                iso_format2=[i.isoformat() for i in new_df[\"Timestamp\"].tolist()]\n",
    "        \n",
    "            \n",
    "                final_x_timestamps=[]\n",
    "                x_values_s=without_iso_format+iso_format2\n",
    "                final_x_timestamps.append(x_values_s)\n",
    "                \n",
    "                \n",
    "                final_y_values=[]\n",
    "                y_values_s=last_30d_df[\"Comp_IB_Bearing_Shaft_Radial_Vib_X_217VI7113A\"].tolist()+new_df[forecast_Column].tolist()\n",
    "                final_y_values.append(y_values_s)\n",
    "            \n",
    "\n",
    "\n",
    "                print(\"length of final_x_timestamps----> \",len(final_x_timestamps[0]))\n",
    "                print(\"length of final_y_values----> \",len(final_y_values[0]))\n",
    "\n",
    "                x_feature_values.append(final_x_timestamps[0])\n",
    "                y_feature_values.append(final_y_values[0])\n",
    "                \n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_agent(query):\n",
    "    prompt = (\n",
    "        \"\"\"\n",
    "            For the following query, if it requires to query a sql table or if the following query askes to plot or forecast the\n",
    "            user is asking to fetch a sql table so, reply as follows:\n",
    "            SQL: <sql_query>;\n",
    "            example:\n",
    "            SQL: SELECT * FROM employees;\n",
    "            SQL: SELECT name, salary FROM employees WHERE em_id = 1;\n",
    "            SQL: SELECT COUNT(*) FROM \"Employee\";\n",
    "            SQL: SELECT * FROM Customer WHERE Country = 'Canada';\n",
    "            If you do not know the answer or if it doesn't passes the first condition of SQL, reply as follows:\n",
    "            PDF: search in pdf.\n",
    "            \"\"\"\n",
    "        + query\n",
    "    )\n",
    "    response = chat_model.invoke(prompt)\n",
    "    #print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# # user_question = \"Plot time trend graph for Comp_Stage_1_Flow_217FC7343A?\"\n",
    "# user_question = \"Compare and Plot time trend graph for Comp_Stage_1_Flow_217FC7343A, Comp_DGS_DE_Primery_Seal_Vent_Diff_217PDI7162 from 2nd feb to 28th feb\"\n",
    "# # user_question = \"Compare and Plot time trend graph for Comp_Stage_1_Flow_217FC7343A, Comp_Stage_1_Suction_Press_217PI7003 from 2nd feb to 28th feb\"\n",
    "# # user_question = \" list asset name and sensorgroup name are in alert status?\"\n",
    "# # user_question = \"Forecast the time trend graph for Comp IB Bearing Shaft Radial Vib X 217VI7113A?\"\n",
    "# x=sql_processing(user_question)\n",
    "\n",
    "\n",
    "# #x==0\n",
    "# # user_question = \"Plot time trend graph for Comp_Stage_1_Flow_217FC7343A?\"n=3\n",
    "# # user_question = \"Compare and Plot time trend graph for Comp_Stage_1_Flow_217FC7343A, Comp_DGS_DE_Primery_Seal_Vent_Diff_217PDI7162 from 2nd feb to 28th feb\"n=1\n",
    "# # user_question = \"Compare and Plot time trend graph for Comp_Stage_1_Flow_217FC7343A, Comp_Stage_1_Suction_Press_217PI7003 from 2nd feb to 28th feb\"n=2\n",
    "\n",
    "\n",
    "# # x==1\n",
    "# # user_question = \" list asset name and sensorgroup name are in alert status?\"x=1\n",
    "\n",
    "\n",
    "# #x==2\n",
    "# # user_question = \"Forecast the time trend graph for Comp IB Bearing Shaft Radial Vib X 217VI7113A?\"\n",
    "\n",
    "\n",
    "\n",
    "# print(\"x values--->\",x)\n",
    "# if x==0:\n",
    "#     question_source = query_agent(user_question)\n",
    " \n",
    "#     pattern ='SQL'\n",
    "#     match = re.search(pattern, str(question_source), re.IGNORECASE)\n",
    "#     if match:\n",
    "#         n=get_sql_query_from_excel(user_question)\n",
    "#         print(\"n values\",n)\n",
    "#     else:   \n",
    "#         print(get_answer_from_pdf(user_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_question = \"Take the last mont data of Comp_Stage_1_Flow_217FC7343A and forecast ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens_eval import Tru\n",
    "# from trulens_eval.tru_custom_app import instrument\n",
    "# tru = Tru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain_google_genai import GoogleGenerativeAI \n",
    "\n",
    "# #oai_client = GoogleGenerativeAI()\n",
    "\n",
    "# class RAG_from_scratch:\n",
    "#     @instrument\n",
    "#     def retrieve(self, query: str) -> list:\n",
    "#         \"\"\"\n",
    "#         Retrieve relevant text from vector store.\n",
    "#         \"\"\"\n",
    "#         retrieved_docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "#         return retrieved_docs[0].page_content\n",
    "#     @instrument\n",
    "#     def generate_completion(self, query: str, context_str: list) -> str:\n",
    "#         \"\"\"\n",
    "#         Generate answer from context.\n",
    "#         completion = oai_client.(\n",
    "#         model=\"gemini-pro\",\n",
    "#         temperature=0,\n",
    "#         messages=\n",
    "#         [\n",
    "#             {\"role\": \"user\",\n",
    "#             \"content\": \n",
    "#             f\"We have provided context information below. \\n\"\n",
    "#             f\"---------------------\\n\"\n",
    "#             f\"{context_str}\"\n",
    "#             f\"\\n---------------------\\n\"\n",
    "#             f\"Given this information, please answer the question: {query}\"\n",
    "#             }\n",
    "#         ]\n",
    "#         ).choices[0].message.content\"\"\" \n",
    "        \n",
    "#         prompt_template = \"\"\"\n",
    "#         Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "#         provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "#         Context:\\n {context}?\\n\n",
    "#         Question: \\n{question}\\n \n",
    "#         Answer: \n",
    "#         \"\"\"\n",
    "#         prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"]) \n",
    "#         qa_retreival = LLMChain(llm=chat_model,prompt=prompt)\n",
    "#         completion = qa_retreival.invoke({\"context\": context_str, \"question\": query})\n",
    "#         print(completion[\"text\"])\n",
    "#         return completion\n",
    "\n",
    "#     @instrument\n",
    "#     def query(self, query: str) -> str:\n",
    "#         context_str = self.retrieve(query)\n",
    "#         completion = self.generate_completion(query, context_str)\n",
    "#         return completion\n",
    "\n",
    "# rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens_eval import Feedback, Select\n",
    "# from trulens_eval.feedback.provider.langchain import Langchain\n",
    "# import numpy as np\n",
    "\n",
    "# provider = Langchain(chain=model)\n",
    "# # Define a groundedness feedback function\n",
    "# f_groundedness = (\n",
    "#     Feedback(provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "#     .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "#     .on_output()\n",
    "# )\n",
    "# # Question/answer relevance between overall question and answer.\n",
    "# f_answer_relevance = (\n",
    "#     Feedback(provider.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "#     .on(Select.RecordCalls.retrieve.args.query)\n",
    "#     .on_output()\n",
    "# )\n",
    "\n",
    "# # Context relevance between question and each context chunk.\n",
    "# f_context_relevance = (\n",
    "#     Feedback(provider.context_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "#     .on(Select.RecordCalls.retrieve.args.query)\n",
    "#     .on(Select.RecordCalls.retrieve.rets)\n",
    "#     .aggregate(np.mean) # choose a different aggregation method if you wish\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens_eval import TruCustomApp\n",
    "# tru_rag = TruCustomApp(rag,\n",
    "#     app_id = 'RAG v1',\n",
    "#     feedbacks = [f_groundedness, f_answer_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tru_rag as recording:\n",
    "#     rag.query(\"what are the correct actions for Centrifugal Compressor Continual Surge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru.get_leaderboard(app_ids=[\"RAG v1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copiolt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:9004\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [21/Sep/2024 16:41:12] \"OPTIONS /receive_data_final_final_video HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Sep/2024 16:41:12] \"POST /receive_data_final_final_video HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received_data-----> {'message': 'Which assets and sensorgroups are in alert state?'}\n",
      " \n",
      "user input-----> Which assets and sensorgroups are in alert state?\n",
      "data [('Turbine Thrust Bearing', '2K1701', 66, '2022-08-20 18:15:00'), ('Compressor Stage 1 Performance', '2K1701', 56, '2022-08-30 10:15:00'), ('Turbine Journal Bearing', '2K1701', 54, '2022-08-30 12:00:00'), ('Turbine Performance', '2K1701', 52, '2022-08-20 13:15:00'), ('Compressor Stage 3 Performance', '2K1701', 42, '2022-08-29 21:30:00'), ('Compressor Stage 2 Performance', '2K1701', 42, '2022-08-27 15:15:00'), ('Compressor Lube Oil System', '2K1701', 38, '2022-03-11 02:00:00'), ('Compressor Journal Bearing', '2K1701', 36, '2022-08-20 18:30:00'), ('LP CASE THRUST BEARING', 'A4101J', 30, '2022-12-19 01:15:00'), ('Compressor STAGE 1 INTERCOOLER SEPERATOR', 'A4101J', 24, '2022-11-02 14:45:00'), ('Compressor Dry Gas Seal', '2K1701', 22, '2022-08-29 10:00:00'), ('LUBE OIL SYSTEM', 'A4101J', 20, '2022-11-17 06:15:00'), ('HP CASE JOURNAL BEARING', 'A4101J', 19, '2022-12-27 02:45:00'), ('Compressor Stage 3 Performance', 'A4101J', 19, '2022-10-31 08:45:00'), ('Turbine Bearing ', 'A4101J', 19, '2022-10-20 05:30:00'), ('HP CASE THRUST BEARING', 'A4101J', 17, '2022-12-28 03:00:00'), ('Compressor Stage 4 Performance', 'A4101J', 15, '2022-07-03 15:15:00'), ('Compressor STAGE 2 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-28 07:00:00'), ('Compressor STAGE 3 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-19 02:00:00'), ('LP CASE JOURNAL BEARING', 'A4101J', 13, '2022-11-01 12:30:00'), ('Turbine Performance', 'A4101J', 12, '2022-10-26 17:15:00'), ('Compressor Thrust Bearing', '2K1701', 12, '2022-03-15 03:45:00'), ('Compressor Stage 2 Performance', 'A4101J', 10, '2022-12-10 22:30:00'), ('Compressor Stage 1 Performance', 'A4101J', 9, '2022-12-03 07:00:00'), ('Turbine Radial Bearing', 'A4101J', 6, '2022-10-31 13:30:00')]\n",
      "item ('Turbine Thrust Bearing', '2K1701', 66, '2022-08-20 18:15:00')\n",
      "item ('Compressor Stage 1 Performance', '2K1701', 56, '2022-08-30 10:15:00')\n",
      "item ('Turbine Journal Bearing', '2K1701', 54, '2022-08-30 12:00:00')\n",
      "item ('Turbine Performance', '2K1701', 52, '2022-08-20 13:15:00')\n",
      "item ('Compressor Stage 3 Performance', '2K1701', 42, '2022-08-29 21:30:00')\n",
      "item ('Compressor Stage 2 Performance', '2K1701', 42, '2022-08-27 15:15:00')\n",
      "item ('Compressor Lube Oil System', '2K1701', 38, '2022-03-11 02:00:00')\n",
      "item ('Compressor Journal Bearing', '2K1701', 36, '2022-08-20 18:30:00')\n",
      "item ('LP CASE THRUST BEARING', 'A4101J', 30, '2022-12-19 01:15:00')\n",
      "item ('Compressor STAGE 1 INTERCOOLER SEPERATOR', 'A4101J', 24, '2022-11-02 14:45:00')\n",
      "item ('Compressor Dry Gas Seal', '2K1701', 22, '2022-08-29 10:00:00')\n",
      "item ('LUBE OIL SYSTEM', 'A4101J', 20, '2022-11-17 06:15:00')\n",
      "item ('HP CASE JOURNAL BEARING', 'A4101J', 19, '2022-12-27 02:45:00')\n",
      "item ('Compressor Stage 3 Performance', 'A4101J', 19, '2022-10-31 08:45:00')\n",
      "item ('Turbine Bearing ', 'A4101J', 19, '2022-10-20 05:30:00')\n",
      "item ('HP CASE THRUST BEARING', 'A4101J', 17, '2022-12-28 03:00:00')\n",
      "item ('Compressor Stage 4 Performance', 'A4101J', 15, '2022-07-03 15:15:00')\n",
      "item ('Compressor STAGE 2 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-28 07:00:00')\n",
      "item ('Compressor STAGE 3 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-19 02:00:00')\n",
      "item ('LP CASE JOURNAL BEARING', 'A4101J', 13, '2022-11-01 12:30:00')\n",
      "item ('Turbine Performance', 'A4101J', 12, '2022-10-26 17:15:00')\n",
      "item ('Compressor Thrust Bearing', '2K1701', 12, '2022-03-15 03:45:00')\n",
      "item ('Compressor Stage 2 Performance', 'A4101J', 10, '2022-12-10 22:30:00')\n",
      "item ('Compressor Stage 1 Performance', 'A4101J', 9, '2022-12-03 07:00:00')\n",
      "item ('Turbine Radial Bearing', 'A4101J', 6, '2022-10-31 13:30:00')\n",
      "('Turbine Thrust Bearing', '2K1701', 66, '2022-08-20 18:15:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 1 Performance', '2K1701', 56, '2022-08-30 10:15:00')\n",
      "\n",
      "\n",
      "('Turbine Journal Bearing', '2K1701', 54, '2022-08-30 12:00:00')\n",
      "\n",
      "\n",
      "('Turbine Performance', '2K1701', 52, '2022-08-20 13:15:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 3 Performance', '2K1701', 42, '2022-08-29 21:30:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 2 Performance', '2K1701', 42, '2022-08-27 15:15:00')\n",
      "\n",
      "\n",
      "('Compressor Lube Oil System', '2K1701', 38, '2022-03-11 02:00:00')\n",
      "\n",
      "\n",
      "('Compressor Journal Bearing', '2K1701', 36, '2022-08-20 18:30:00')\n",
      "\n",
      "\n",
      "('LP CASE THRUST BEARING', 'A4101J', 30, '2022-12-19 01:15:00')\n",
      "\n",
      "\n",
      "('Compressor STAGE 1 INTERCOOLER SEPERATOR', 'A4101J', 24, '2022-11-02 14:45:00')\n",
      "\n",
      "\n",
      "('Compressor Dry Gas Seal', '2K1701', 22, '2022-08-29 10:00:00')\n",
      "\n",
      "\n",
      "('LUBE OIL SYSTEM', 'A4101J', 20, '2022-11-17 06:15:00')\n",
      "\n",
      "\n",
      "('HP CASE JOURNAL BEARING', 'A4101J', 19, '2022-12-27 02:45:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 3 Performance', 'A4101J', 19, '2022-10-31 08:45:00')\n",
      "\n",
      "\n",
      "('Turbine Bearing ', 'A4101J', 19, '2022-10-20 05:30:00')\n",
      "\n",
      "\n",
      "('HP CASE THRUST BEARING', 'A4101J', 17, '2022-12-28 03:00:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 4 Performance', 'A4101J', 15, '2022-07-03 15:15:00')\n",
      "\n",
      "\n",
      "('Compressor STAGE 2 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-28 07:00:00')\n",
      "\n",
      "\n",
      "('Compressor STAGE 3 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-19 02:00:00')\n",
      "\n",
      "\n",
      "('LP CASE JOURNAL BEARING', 'A4101J', 13, '2022-11-01 12:30:00')\n",
      "\n",
      "\n",
      "('Turbine Performance', 'A4101J', 12, '2022-10-26 17:15:00')\n",
      "\n",
      "\n",
      "('Compressor Thrust Bearing', '2K1701', 12, '2022-03-15 03:45:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 2 Performance', 'A4101J', 10, '2022-12-10 22:30:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 1 Performance', 'A4101J', 9, '2022-12-03 07:00:00')\n",
      "\n",
      "\n",
      "('Turbine Radial Bearing', 'A4101J', 6, '2022-10-31 13:30:00')\n",
      "\n",
      "\n",
      "x value---> 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Sep/2024 16:49:55] \"OPTIONS /receive_data_final_final_video HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Sep/2024 16:49:55] \"POST /receive_data_final_final_video HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received_data-----> {'message': 'Which assets and sensorgroups are in alert state?'}\n",
      " \n",
      "user input-----> Which assets and sensorgroups are in alert state?\n",
      "data [('Turbine Thrust Bearing', '2K1701', 66, '2022-08-20 18:15:00'), ('Compressor Stage 1 Performance', '2K1701', 56, '2022-08-30 10:15:00'), ('Turbine Journal Bearing', '2K1701', 54, '2022-08-30 12:00:00'), ('Turbine Performance', '2K1701', 52, '2022-08-20 13:15:00'), ('Compressor Stage 3 Performance', '2K1701', 42, '2022-08-29 21:30:00'), ('Compressor Stage 2 Performance', '2K1701', 42, '2022-08-27 15:15:00'), ('Compressor Lube Oil System', '2K1701', 38, '2022-03-11 02:00:00'), ('Compressor Journal Bearing', '2K1701', 36, '2022-08-20 18:30:00'), ('LP CASE THRUST BEARING', 'A4101J', 30, '2022-12-19 01:15:00'), ('Compressor STAGE 1 INTERCOOLER SEPERATOR', 'A4101J', 24, '2022-11-02 14:45:00'), ('Compressor Dry Gas Seal', '2K1701', 22, '2022-08-29 10:00:00'), ('LUBE OIL SYSTEM', 'A4101J', 20, '2022-11-17 06:15:00'), ('HP CASE JOURNAL BEARING', 'A4101J', 19, '2022-12-27 02:45:00'), ('Compressor Stage 3 Performance', 'A4101J', 19, '2022-10-31 08:45:00'), ('Turbine Bearing ', 'A4101J', 19, '2022-10-20 05:30:00'), ('HP CASE THRUST BEARING', 'A4101J', 17, '2022-12-28 03:00:00'), ('Compressor Stage 4 Performance', 'A4101J', 15, '2022-07-03 15:15:00'), ('Compressor STAGE 2 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-28 07:00:00'), ('Compressor STAGE 3 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-19 02:00:00'), ('LP CASE JOURNAL BEARING', 'A4101J', 13, '2022-11-01 12:30:00'), ('Turbine Performance', 'A4101J', 12, '2022-10-26 17:15:00'), ('Compressor Thrust Bearing', '2K1701', 12, '2022-03-15 03:45:00'), ('Compressor Stage 2 Performance', 'A4101J', 10, '2022-12-10 22:30:00'), ('Compressor Stage 1 Performance', 'A4101J', 9, '2022-12-03 07:00:00'), ('Turbine Radial Bearing', 'A4101J', 6, '2022-10-31 13:30:00')]\n",
      "item ('Turbine Thrust Bearing', '2K1701', 66, '2022-08-20 18:15:00')\n",
      "item ('Compressor Stage 1 Performance', '2K1701', 56, '2022-08-30 10:15:00')\n",
      "item ('Turbine Journal Bearing', '2K1701', 54, '2022-08-30 12:00:00')\n",
      "item ('Turbine Performance', '2K1701', 52, '2022-08-20 13:15:00')\n",
      "item ('Compressor Stage 3 Performance', '2K1701', 42, '2022-08-29 21:30:00')\n",
      "item ('Compressor Stage 2 Performance', '2K1701', 42, '2022-08-27 15:15:00')\n",
      "item ('Compressor Lube Oil System', '2K1701', 38, '2022-03-11 02:00:00')\n",
      "item ('Compressor Journal Bearing', '2K1701', 36, '2022-08-20 18:30:00')\n",
      "item ('LP CASE THRUST BEARING', 'A4101J', 30, '2022-12-19 01:15:00')\n",
      "item ('Compressor STAGE 1 INTERCOOLER SEPERATOR', 'A4101J', 24, '2022-11-02 14:45:00')\n",
      "item ('Compressor Dry Gas Seal', '2K1701', 22, '2022-08-29 10:00:00')\n",
      "item ('LUBE OIL SYSTEM', 'A4101J', 20, '2022-11-17 06:15:00')\n",
      "item ('HP CASE JOURNAL BEARING', 'A4101J', 19, '2022-12-27 02:45:00')\n",
      "item ('Compressor Stage 3 Performance', 'A4101J', 19, '2022-10-31 08:45:00')\n",
      "item ('Turbine Bearing ', 'A4101J', 19, '2022-10-20 05:30:00')\n",
      "item ('HP CASE THRUST BEARING', 'A4101J', 17, '2022-12-28 03:00:00')\n",
      "item ('Compressor Stage 4 Performance', 'A4101J', 15, '2022-07-03 15:15:00')\n",
      "item ('Compressor STAGE 2 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-28 07:00:00')\n",
      "item ('Compressor STAGE 3 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-19 02:00:00')\n",
      "item ('LP CASE JOURNAL BEARING', 'A4101J', 13, '2022-11-01 12:30:00')\n",
      "item ('Turbine Performance', 'A4101J', 12, '2022-10-26 17:15:00')\n",
      "item ('Compressor Thrust Bearing', '2K1701', 12, '2022-03-15 03:45:00')\n",
      "item ('Compressor Stage 2 Performance', 'A4101J', 10, '2022-12-10 22:30:00')\n",
      "item ('Compressor Stage 1 Performance', 'A4101J', 9, '2022-12-03 07:00:00')\n",
      "item ('Turbine Radial Bearing', 'A4101J', 6, '2022-10-31 13:30:00')\n",
      "('Turbine Thrust Bearing', '2K1701', 66, '2022-08-20 18:15:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 1 Performance', '2K1701', 56, '2022-08-30 10:15:00')\n",
      "\n",
      "\n",
      "('Turbine Journal Bearing', '2K1701', 54, '2022-08-30 12:00:00')\n",
      "\n",
      "\n",
      "('Turbine Performance', '2K1701', 52, '2022-08-20 13:15:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 3 Performance', '2K1701', 42, '2022-08-29 21:30:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 2 Performance', '2K1701', 42, '2022-08-27 15:15:00')\n",
      "\n",
      "\n",
      "('Compressor Lube Oil System', '2K1701', 38, '2022-03-11 02:00:00')\n",
      "\n",
      "\n",
      "('Compressor Journal Bearing', '2K1701', 36, '2022-08-20 18:30:00')\n",
      "\n",
      "\n",
      "('LP CASE THRUST BEARING', 'A4101J', 30, '2022-12-19 01:15:00')\n",
      "\n",
      "\n",
      "('Compressor STAGE 1 INTERCOOLER SEPERATOR', 'A4101J', 24, '2022-11-02 14:45:00')\n",
      "\n",
      "\n",
      "('Compressor Dry Gas Seal', '2K1701', 22, '2022-08-29 10:00:00')\n",
      "\n",
      "\n",
      "('LUBE OIL SYSTEM', 'A4101J', 20, '2022-11-17 06:15:00')\n",
      "\n",
      "\n",
      "('HP CASE JOURNAL BEARING', 'A4101J', 19, '2022-12-27 02:45:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 3 Performance', 'A4101J', 19, '2022-10-31 08:45:00')\n",
      "\n",
      "\n",
      "('Turbine Bearing ', 'A4101J', 19, '2022-10-20 05:30:00')\n",
      "\n",
      "\n",
      "('HP CASE THRUST BEARING', 'A4101J', 17, '2022-12-28 03:00:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 4 Performance', 'A4101J', 15, '2022-07-03 15:15:00')\n",
      "\n",
      "\n",
      "('Compressor STAGE 2 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-28 07:00:00')\n",
      "\n",
      "\n",
      "('Compressor STAGE 3 INTERCOOLER SEPERATOR', 'A4101J', 13, '2022-12-19 02:00:00')\n",
      "\n",
      "\n",
      "('LP CASE JOURNAL BEARING', 'A4101J', 13, '2022-11-01 12:30:00')\n",
      "\n",
      "\n",
      "('Turbine Performance', 'A4101J', 12, '2022-10-26 17:15:00')\n",
      "\n",
      "\n",
      "('Compressor Thrust Bearing', '2K1701', 12, '2022-03-15 03:45:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 2 Performance', 'A4101J', 10, '2022-12-10 22:30:00')\n",
      "\n",
      "\n",
      "('Compressor Stage 1 Performance', 'A4101J', 9, '2022-12-03 07:00:00')\n",
      "\n",
      "\n",
      "('Turbine Radial Bearing', 'A4101J', 6, '2022-10-31 13:30:00')\n",
      "\n",
      "\n",
      "x value---> 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,request,jsonify\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import os\n",
    "import ast\n",
    "from datetime import datetime\n",
    "# import statistics\n",
    "current_time = datetime.now().strftime(\"%I:%M %p\")\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def Home():\n",
    "    return \"Apis Runnning\"\n",
    "\n",
    "@app.route('/receive_data_final_final_video', methods=['POST'])\n",
    "def receive_data():\n",
    "    try:\n",
    "        if request.method == 'POST':\n",
    "            received_data = request.json  # Get the JSON data sent from frontend\n",
    "            print(\"received_data----->\", received_data)\n",
    "            print(\" \")\n",
    "            \n",
    "            \n",
    "            \n",
    "            user_question=received_data[\"message\"]\n",
    "            \n",
    "            \n",
    "            print(\"user input----->\", user_question)\n",
    "\n",
    "            x=sql_processing(user_question)\n",
    "            print(\"x value--->\",x)   \n",
    "            \n",
    "            \n",
    "             \n",
    "            \n",
    "\n",
    "            if x==2:\n",
    "                print(\"x feature selection-------->\",len(x_feature_values[0]),\"---->\",x_feature_values[0])\n",
    "                print(\"y feature selection-------->\",len(y_feature_values[0]),\"---->\",y_feature_values[0])\n",
    "   \n",
    "                received_data.update({'role': 'assistant', 'image':  base64_encoded_img1,\n",
    "                                      'x_values_feature':x_feature_values[0],\n",
    "                                      'y_values_feature':y_feature_values[0],\n",
    "                                      'sentTime':current_time})\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "#             if x==1 and base64_encoded_img1[0]!=\"{\" :\n",
    "#                 z_values=[]\n",
    "#                 for i in y_values[0]:\n",
    "#                     if str(i).strip()==\"nan\":\n",
    "#                         z_values.append('0')\n",
    "#                     else:\n",
    "#                         z_values.append(i)\n",
    "\n",
    "\n",
    "\n",
    "#                 y_values_graphs=list(map(float,z_values))\n",
    "#                 received_data.update({'role': 'assistant', 'image':  base64_encoded_img1,'x_values':x_values[0],'y_values':y_values_graphs,'sentTime':current_time})\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "            if x==1 and base64_encoded_img1[0]==\"{\" :\n",
    "                received_data.update({'role': 'assistant', 'table_content':  ast.literal_eval(base64_encoded_img1),'sentTime':current_time})\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            if x==0:\n",
    "                question_source = query_agent(user_question)\n",
    "                pattern ='SQL'\n",
    "                match = re.search(pattern, str(question_source), re.IGNORECASE)\n",
    "                \n",
    "                if match:\n",
    "                    n=get_sql_query_from_excel(user_question)\n",
    "                    \n",
    "                    \n",
    "                    if n==1:\n",
    "                        z1_compare_plots=[]\n",
    "                        for i in x1_values_compare_plot[0]:\n",
    "                            if str(i).strip()==\"nan\":\n",
    "                                z1_compare_plots.append('0')\n",
    "                            else:\n",
    "                                z1_compare_plots.append(i)\n",
    "                        x1_compare_value_plot_final=list(map(float,z1_compare_plots))\n",
    "                        \n",
    "                        \n",
    "                        z2_compare_plots=[]\n",
    "                        \n",
    "                        for i in x11_values_compare_plot[0]:\n",
    "#                             print(\"i------->\",i)\n",
    "                            if str(i).strip()==\"nan\":\n",
    "                                z2_compare_plots.append('0')\n",
    "                            else:\n",
    "                                z2_compare_plots.append(i)\n",
    "                                \n",
    "                        x11_compare_value_plot_final=list(map(float,z2_compare_plots))  \n",
    "                        compare_graph1=\"compare_graphs\"\n",
    "                        \n",
    "#                         print(\"x1_values_comp1\",x1_compare_value_plot_final)\n",
    "#                         print(\"x2_values_comp1\",x11_compare_value_plot_final)\n",
    "#                         print(\"y1_values_comp1\",y1_values_compare_plot[0])\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        received_data.update({'role': 'assistant','compare_graph1':compare_graph1,\n",
    "                                              'x1_values_comp1':x1_compare_value_plot_final,\n",
    "                                              'x2_values_comp2':x11_compare_value_plot_final,\n",
    "                                              'y1_values_comp':y1_values_compare_plot[0],\n",
    "                                              \n",
    "                                              'sentTime':current_time})\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    if n==2:\n",
    "                        \n",
    "\n",
    "\n",
    "                        zz1_compare_plots=[]\n",
    "                        for i in x2_values_compare_plot[0]:\n",
    "                            if str(i).strip()==\"nan\":\n",
    "                                zz1_compare_plots.append('0')\n",
    "                            else:\n",
    "                                zz1_compare_plots.append(i)\n",
    "                        xx1_compare_value_plot_final=list(map(float,zz1_compare_plots))\n",
    "                        \n",
    "                        \n",
    "                        zz2_compare_plots=[]\n",
    "                        \n",
    "                        for i in x22_values_compare_plot[0]:\n",
    "#                             print(\"i------->\",i)\n",
    "                            if str(i).strip()==\"nan\":\n",
    "                                zz2_compare_plots.append('0')\n",
    "                            else:\n",
    "                                zz2_compare_plots.append(i)\n",
    "                                \n",
    "                        xx11_compare_value_plot_final=list(map(float,zz2_compare_plots))  \n",
    "                        compare_graph2=\"compare_graphs\"\n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        received_data.update({'role': 'assistant','compare_graph2':compare_graph2,\n",
    "                                              'xx1_values_comp1':xx1_compare_value_plot_final,\n",
    "                                              'xx2_values_comp1':xx11_compare_value_plot_final,\n",
    "                                              'y2_values_comp1':y2_values_compare_plot[0],\n",
    "                                              'sentTime':current_time})\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    if n==3:\n",
    "                        print(\"n values\",n)\n",
    "                        z_values=[]\n",
    "                        print(\"y values\",y_values_plot)\n",
    "                        for i in y_values_plot[0]:\n",
    "                            if str(i).strip()==\"nan\":\n",
    "                                z_values.append('0')\n",
    "                            else:\n",
    "                                z_values.append(i)\n",
    "\n",
    "\n",
    "\n",
    "                        y_values_graphs=list(map(float,z_values))\n",
    "                        received_data.update({'role': 'assistant', 'image':  base64_encoded_img1,'x_values':x_values_plot[0],'y_values':y_values_graphs,'sentTime':current_time})\n",
    "                else:   \n",
    "                    agent_output2=get_answer_from_pdf(user_question)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#                     #sample\n",
    "#                     document=retriever.get_relevant_documents(user_question)\n",
    "#                     for doc in document:\n",
    "#                         agent_output2=agent_output2+\"\\n\"+\"(\"+doc.metadata[\"source\"]+\")\"\n",
    "#                     #sample         \n",
    "                        \n",
    "                    received_data.update({'role': 'assistant', 'x': agent_output2,'pdfpath':\"C:/Users/40019115/geminiAPI/Centrifugal Air Compressor Troubleshooting Guide 2.pdf\",'sentTime':current_time})\n",
    "            return jsonify(received_data)\n",
    "           \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    " \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return jsonify({\"message\": \"Error processing data\"}), 500\n",
    "if __name__ == '__main__':\n",
    "#     app.run(threaded=False)\n",
    "    app.run(host='127.0.0.1', port=9004, debug=False,threaded=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
